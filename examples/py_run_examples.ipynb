{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setting Up the Python Path\n",
    "\n",
    "This snippet appends the root directory to ensure that the corresponding Python module can be imported and accessed in the current session.\n",
    "\n",
    "The following code adds the root directory to Python's module search path to enable importing other modules or packages within the project.\n",
    "\n"
   ],
   "id": "cf56288c2d2db84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory of the current directory\n",
    "root_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "src_dir = os.path.join(root_dir, 'src')\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)"
   ],
   "id": "e30ebd8c439cec7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Installation of Dependencies\n",
    "\n",
    "run `pip install -r requirements.txt`\n",
    "\n",
    "The command pip install -r requirements.txt installs all the necessary Python packages listed in the requirements.txt file, which defines the dependencies needed for the project to function correctly."
   ],
   "id": "b9fd9937b69df9ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Importing Modules from the Rapyton Package\n",
    "\n",
    "In this section, we import essential modules from the `src.rapython` package. This allows us to access various functionalities for evaluation, semi-supervised learning, supervised learning, and unsupervised learning.\n",
    "\n",
    "- `evaluation`: Contains functions and classes for performance evaluation.\n",
    "- `semi`: Implements semi-supervised rank aggregation algorithms.\n",
    "- `supervised`: Includes supervised rank aggregation algorithms.\n",
    "- `unsupervised`: Provides unsupervised rank aggregation algorithms.\n"
   ],
   "id": "b0c4f5e79feef006"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import glob\n",
    "\n",
    "from src.rapython.evaluation import *\n",
    "from src.rapython.semi import *\n",
    "from src.rapython.supervised import *\n",
    "from src.rapython.unsupervised import *"
   ],
   "id": "e978211b8bf3684e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " # Running Unsupervised RA Methods\n",
    "\n",
    "The following code sets the path to the dataset file and the output file directory required for the unsupervised approach"
   ],
   "id": "d4643f4fbd8e542d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:58:57.172616Z",
     "start_time": "2024-10-25T08:58:57.168401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_file_path = '..\\\\test\\\\full_lists\\\\data\\\\simulation_test.csv'\n",
    "output_base_path = '..\\\\test\\\\full_lists\\\\results'"
   ],
   "id": "69e32796245b83b7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code snippet sequentially calls various functions related to the evaluation of ranking methods, processing each one while printing status messages to the console. For each function, it constructs the output file path based on the specified `output_base_path` and the name of the function being called, then executes the function with the `input_file_path` and the constructed output path. After each function call, a message indicating its completion is printed. Finally, a summary message confirms that all functions have been processed successfully.\n",
   "id": "4b188fd53818ce5b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "print(\"Starting to process all functions...\\n\")\n",
    "\n",
    "print(\"Calling bordacount()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{bordacount.__name__}.csv\")\n",
    "bordacount(input_file_path, output_file_path)\n",
    "print(\"Finished bordacount()\\n\")\n",
    "\n",
    "print(\"Calling borda_score()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{borda_score.__name__}.csv\")\n",
    "borda_score(input_file_path, output_file_path)\n",
    "print(\"Finished borda_score()\\n\")\n",
    "\n",
    "print(\"Calling cg()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{cg.__name__}.csv\")\n",
    "cg(input_file_path, output_file_path)\n",
    "print(\"Finished cg()\\n\")\n",
    "\n",
    "print(\"Calling combanz()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{combanz.__name__}.csv\")\n",
    "combanz(input_file_path, output_file_path)\n",
    "print(\"Finished combanz()\\n\")\n",
    "\n",
    "print(\"Calling combmax()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{combmax.__name__}.csv\")\n",
    "combmax(input_file_path, output_file_path)\n",
    "print(\"Finished combmax()\\n\")\n",
    "\n",
    "print(\"Calling combmed()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{combmed.__name__}.csv\")\n",
    "combmed(input_file_path, output_file_path)\n",
    "print(\"Finished combmed()\\n\")\n",
    "\n",
    "print(\"Calling combmin()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{combmin.__name__}.csv\")\n",
    "combmin(input_file_path, output_file_path)\n",
    "print(\"Finished combmin()\\n\")\n",
    "\n",
    "print(\"Calling combsum()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{combsum.__name__}.csv\")\n",
    "combsum(input_file_path, output_file_path)\n",
    "print(\"Finished combsum()\\n\")\n",
    "\n",
    "print(\"Calling dibra()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{dibra.__name__}.csv\")\n",
    "dibra(input_file_path, output_file_path, InputType.RANK)\n",
    "print(\"Finished dibra()\\n\")\n",
    "\n",
    "print(\"Calling dowdall()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{dowdall.__name__}.csv\")\n",
    "dowdall(input_file_path, output_file_path)\n",
    "print(\"Finished dowdall()\\n\")\n",
    "\n",
    "print(\"Calling er()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{er.__name__}.csv\")\n",
    "er(input_file_path, output_file_path, InputType.RANK)\n",
    "print(\"Finished er()\\n\")\n",
    "\n",
    "print(\"Calling hpa()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{hpa.__name__}.csv\")\n",
    "hpa(input_file_path, output_file_path, InputType.RANK)\n",
    "print(\"Finished hpa()\\n\")\n",
    "\n",
    "print(\"Calling irank()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{irank.__name__}.csv\")\n",
    "irank(input_file_path, output_file_path, InputType.RANK)\n",
    "print(\"Finished irank()\\n\")\n",
    "\n",
    "print(\"Calling markovchainmethod()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{markovchainmethod.__name__}.csv\")\n",
    "markovchainmethod(input_file_path, output_file_path, mc_type=McType.MC1)\n",
    "print(\"Finished markovchainmethod()\\n\")\n",
    "\n",
    "print(\"Calling mean()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{mean.__name__}.csv\")\n",
    "mean(input_file_path, output_file_path)\n",
    "print(\"Finished mean()\\n\")\n",
    "\n",
    "print(\"Calling median()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{median.__name__}.csv\")\n",
    "median(input_file_path, output_file_path)\n",
    "print(\"Finished median()\\n\")\n",
    "\n",
    "print(\"Calling mork_heuristic()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{mork_heuristic.__name__}.csv\")\n",
    "mork_heuristic(input_file_path, output_file_path)\n",
    "print(\"Finished mork_heuristic()\\n\")\n",
    "\n",
    "print(\"Calling postndcg()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{postndcg.__name__}.csv\")\n",
    "postndcg(input_file_path, output_file_path, input_type=InputType.RANK)\n",
    "print(\"Finished postndcg()\\n\")\n",
    "\n",
    "print(\"Calling rrf()...\")\n",
    "output_file_path = os.path.join(output_base_path, f\"{rrf.__name__}.csv\")\n",
    "rrf(input_file_path, output_file_path)\n",
    "print(\"Finished rrf()\\n\")\n",
    "\n",
    "print(\"All functions processed successfully!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Run Supervised RA Methods\n",
    "\n",
    "In this code snippet, file paths for the training and testing datasets are defined. The `train_file_path` variable points to the CSV file containing the training data, while the `train_rel_path` variable specifies the corresponding relevance data for the training set. Similarly, `test_file_path` is assigned the path to the test dataset, and `test_rel_path` points to its relevance data. These file paths will be used later in the script for model training and evaluation."
   ],
   "id": "1104776e0ec3a4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_file_path = '..\\\\test\\\\full_lists\\\\data\\\\simulation_train.csv'\n",
    "train_rel_path = '..\\\\test\\\\full_lists\\\\data\\\\simulation_train_rel.csv'\n",
    "\n",
    "test_file_path = '..\\\\test\\\\full_lists\\\\data\\\\simulation_test.csv'\n",
    "test_rel_path = '..\\\\test\\\\full_lists\\\\data\\\\simulation_test_rel.csv'"
   ],
   "id": "510cf308d95cf6be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code snippet initializes and executes various ranking methods, including AggRankDE, CRF, Weighted Borda, and different IRA methods.\n",
    "\n",
    "1. **AggRankDE**: The code first initializes the AggRankDE method and then trains it using the training data provided by `train_file_path` and `train_rel_path`. After training, it tests the model on the test dataset, saving the results to `aggrankde.csv`.\n",
    "\n",
    "2. **CRF**: Next, the CRF method is initialized and trained for a specified number of epochs (2 in this case). It is then tested on the test dataset, with results saved to `crf.csv`.\n",
    "\n",
    "3. **Weighted Borda**: The Weighted Borda method is also initialized and trained using the same training data. The results of the testing phase are saved to `weightedborda.csv`.\n",
    "\n",
    "4. **IRA Methods**: The snippet also calls two variations of the IRA method: `IRA_RANK` and `IRA_SCORE`. Each method is executed with the test dataset, and results are saved to their respective CSV files (`ira_rank.csv` and `ira_score.csv`).\n",
    "\n",
    "5. **QI_IRA Method**: Finally, the QI_IRA method is called with its own parameters, and results are saved to `qi_ira.csv`.\n",
    "\n",
    "Throughout the execution, the process is logged to the console to indicate the initialization, training, and testing stages of each method.\n"
   ],
   "id": "a267e28b16c11a44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Initializing AggRankDE...\")\n",
    "aggRankDE = AggRankDE()\n",
    "print(\"AggRankDE initialized.\")\n",
    "\n",
    "print(\"Training AggRankDE...\")\n",
    "aggRankDE.train(train_file_path, train_rel_path, InputType.RANK)\n",
    "print(\"AggRankDE training completed.\")\n",
    "\n",
    "print(\"Testing AggRankDE...\")\n",
    "test_output_path = os.path.join(output_base_path, 'aggrankde.csv')\n",
    "aggRankDE.test(test_file_path, test_output_path)\n",
    "print(\"AggRankDE testing completed.\\n\")\n",
    "\n",
    "print(\"Initializing CRF...\")\n",
    "crf = CRF()\n",
    "print(\"CRF initialized.\")\n",
    "\n",
    "print(\"Training CRF...\")\n",
    "crf.train(train_file_path, train_rel_path, InputType.RANK, epoch=2)\n",
    "print(\"CRF training completed.\")\n",
    "\n",
    "print(\"Testing CRF...\")\n",
    "test_output_path = os.path.join(output_base_path, 'crf.csv')\n",
    "crf.test(test_file_path, test_output_path)\n",
    "print(\"CRF testing completed.\\n\")\n",
    "\n",
    "weightborda = WeightedBorda()\n",
    "print(\"Training WeightedBorda method...\")\n",
    "weightborda.train(train_file_path, train_rel_path)\n",
    "print(\"WeightedBorda training completed.\")\n",
    "\n",
    "print(\"Testing WeightedBorda...\")\n",
    "test_output_path = os.path.join(output_base_path, 'weightedborda.csv')\n",
    "weightborda.test(test_file_path, test_output_path)\n",
    "print(\"WeightedBorda testing completed.\\n\")\n",
    "\n",
    "print(\"Calling IRA method (IRA_RANK)...\")\n",
    "test_output_path = os.path.join(output_base_path, 'ira_rank.csv')\n",
    "ira(test_file_path, test_output_path, test_rel_path, 3, 2, 0.02, MethodType.IRA_RANK, InputType.RANK)\n",
    "print(\"IRA method (IRA_RANK) completed.\")\n",
    "\n",
    "print(\"Calling IRA method (IRA_SCORE)...\")\n",
    "test_output_path = os.path.join(output_base_path, 'ira_score.csv')\n",
    "ira(test_file_path, test_output_path, test_rel_path, 3, 2, 0.02, MethodType.IRA_SCORE, InputType.RANK)\n",
    "print(\"IRA method (IRA_SCORE) completed.\\n\")\n",
    "\n",
    "print(\"Calling QI_IRA method...\")\n",
    "test_output_path = os.path.join(output_base_path, 'qi_ira.csv')\n",
    "qi_ira(test_file_path, test_output_path, test_rel_path, 3, 2, 0.02, InputType.RANK)\n",
    "print(\"QI_IRA method completed.\")\n"
   ],
   "id": "8c0e34d686dcae13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Run Semi-Supervised RA Methods\n",
    "\n",
    "This code snippet demonstrates the initialization, training, and testing of the SSRA (Semi-Supervised Rank Aggregation) method.\n",
    "\n",
    "1. **Initialization**: The SSRA method is initialized by creating an instance of the `SSRA` class. A message confirming the successful initialization is printed to the console.\n",
    "\n",
    "2. **Training**: The method is then trained using the specified training dataset located at `train_file_path` and the corresponding relevance information at `train_rel_path`. The training is conducted with the input type set to rank (`InputType.RANK`). Upon completion, a message is printed to indicate that the training process is finished.\n",
    "\n",
    "3. **Testing**: After training, the SSRA method is tested with the test dataset found at `test_file_path`. The results are saved to a CSV file named `ssra.csv` in the specified output directory. A message confirming the successful completion of the testing process is printed to the console.\n"
   ],
   "id": "3dde63e5d30794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Initializing SSRA...\")\n",
    "ssra = SSRA()\n",
    "print(\"SSRA initialized.\")\n",
    "\n",
    "print(\"Training SSRA...\")\n",
    "ssra.train(train_file_path, train_rel_path, InputType.RANK)\n",
    "print(\"SSRA training completed.\")\n",
    "\n",
    "print(\"Testing SSRA...\")\n",
    "test_output_path = os.path.join(output_base_path, 'ssra.csv')\n",
    "ssra.test(test_file_path, test_output_path)\n",
    "print(\"SSRA testing completed.\")\n"
   ],
   "id": "eeb9488b03646f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation of Algorithm Results\n",
    "\n",
    "This code snippet sets up the evaluation of ranking results from CSV files stored in a specified folder.\n",
    "\n",
    "1. **Folder Path Definition**: The variable `folder_path` is initialized with the path to the directory containing the result CSV files.\n",
    "\n",
    "2. **Retrieving CSV Files**: Using the `glob` module, the code retrieves all files with a `.csv` extension from the specified folder. The file paths are stored in the `csv_files` list for further processing.\n",
    "\n",
    "3. **Loading Relevance Scores**: The relevance scores for the test dataset are loaded from a CSV file located at `..\\\\test\\\\full_lists\\\\data\\\\simulation_test_rel.csv`. This file contains the relevance information that will be used in the evaluation.\n",
    "\n",
    "4. **Evaluation Setup**: An instance of the `Evaluation` class is created to facilitate the assessment of the ranking results.\n",
    "\n",
    "5. **Results DataFrame Initialization**: An empty DataFrame named `results_df` is created with specified columns: 'File Name', 'mAP@10', 'NDCG@10', and 'Rank@1'. This DataFrame will be used to store the evaluation results of the ranking methods applied to the CSV files.\n"
   ],
   "id": "80b8c001fd1d0bff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "folder_path = '..\\\\test\\\\full_lists\\\\results'\n",
    "\n",
    "# 获取文件夹中的所有 .csv 文件\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# 加载相关性分数文件\n",
    "rel_data = pd.read_csv('..\\\\test\\\\full_lists\\\\data\\\\simulation_test_rel.csv', header=None)\n",
    "\n",
    "evaluation = Evaluation()\n",
    "\n",
    "# 创建一个空的 DataFrame 来存储评估结果\n",
    "results_df = pd.DataFrame(columns=['File Name', 'mAP@10', 'NDCG@10', 'Rank@1'])"
   ],
   "id": "cf528551161deadd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code iterates over a list of CSV files containing ranking results, evaluates each file using multiple metrics, and stores the results in both a DataFrame and an Excel file.\n",
    "\n",
    "1. **File Processing Loop**: For each CSV file in `csv_files`, the code extracts the file name (without the extension) for identification in the evaluation report.\n",
    "\n",
    "2. **Evaluation Metrics Calculation**: The code calculates three evaluation metrics:\n",
    "   - `mAP@10` (Mean Average Precision at 10)\n",
    "   - `NDCG@10` (Normalized Discounted Cumulative Gain at 10)\n",
    "   - `Rank@1` (Rank position of the first relevant item)\n",
    "   These metrics assess the accuracy and relevance of the ranking output based on `rel_data`.\n",
    "\n",
    "3. **Results Storage**: Each file's evaluation metrics are printed and appended as a new row in `results_df`, which consolidates results across all processed files.\n",
    "\n",
    "4. **Excel Output**: Once all files are evaluated, the accumulated results in `results_df` are saved as an Excel file at `..\\\\test\\\\full_lists\\\\results\\\\evaluation_results.xlsx`, enabling easy review and analysis of the evaluation results.\n"
   ],
   "id": "34af998b9bed3ff2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for file in csv_files:\n",
    "    # 获取不带后缀的文件名\n",
    "    file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "    # 加载csv文件\n",
    "    result_data = pd.read_csv(file, header=None)\n",
    "\n",
    "    # 计算评估结果\n",
    "    map_at_10 = evaluation.eval_mean_average_precision(result_data, rel_data, 10)\n",
    "    ndcg_at_10 = evaluation.eval_ndcg(result_data, rel_data, 10)\n",
    "    rank_at_1 = evaluation.eval_rank(result_data, rel_data, 1)\n",
    "\n",
    "    # 打印评估结果\n",
    "    print(f\"{file_name}: mAP@10: {map_at_10}, NDCG@10: {ndcg_at_10}, Rank@1: {rank_at_1}\")\n",
    "\n",
    "    # 将评估结果作为一行添加到 DataFrame\n",
    "    new_row = pd.DataFrame({\n",
    "        'File Name': [file_name],\n",
    "        'mAP@10': [map_at_10],\n",
    "        'NDCG@10': [ndcg_at_10],\n",
    "        'Rank@1': [rank_at_1]\n",
    "    })\n",
    "\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# 将评估结果写入 Excel 文件\n",
    "output_excel_path = '..\\\\test\\\\full_lists\\\\results\\\\evaluation_results.xlsx'\n",
    "results_df.to_excel(output_excel_path, index=False, sheet_name='sheet1')\n",
    "\n",
    "print(f\"Evaluation results have been written to {output_excel_path}\")\n"
   ],
   "id": "6a78ce3c9fdac8dd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
